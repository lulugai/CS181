\documentclass{harvardml}

% Authors: Amir Shanehsazzadeh, Andrew Kim, Nari Johnson
% January 2021

% Adapted from CS281 Fall 2019 section 0 notes

% This tex file relies on
% the presence of two files:
% harvardml.cls and common.sty

\course{CS181-s18}
\assignment{CS181 Pset 0 Solutions}
\duedate{Never}

\usepackage{hyperref}
\usepackage{url}
\usepackage{amsfonts, amsmath, amsthm}
\usepackage{listings}
\usepackage[shortlabels]{enumitem}

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\theoremstyle{plain}
\usepackage[textsize=tiny]{todonotes}

% Some useful macros.
\newcommand{\given}{\,|\,}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\var}{\text{Var}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\p}{\partial}
\newcommand{\mba}{\mathbf{a}}
\newcommand{\mbb}{\mathbf{b}}
\newcommand{\mbx}{\mathbf{x}}
\newcommand{\mcX}{\mathcal{X}}
\newcommand{\mcY}{\mathcal{Y}}
\newcommand{\boldw}{\mathbf{w}}
\newcommand{\mbxt}{\tilde{\mathbf{x}}}
\newcommand{\Sigmat}{\tilde{\Sigma}}
\newcommand{\mbz}{\mathbf{z}}
\newcommand{\mbw}{\mathbf{w}}
\newcommand{\mcN}{\mathcal{N}}
\newcommand{\mcP}{\mathcal{P}}
\newcommand{\eps}{\epsilon}
\newcommand{\trans}{\intercal}
\newcommand{\Ut}{\tilde{U}}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\angstrom}{\textup{\AA}}
\renewcommand{\v}[1]{\mathbf{#1}}

\begin{document}
\begin{center} The goal of these problems is to quickly check your readiness for CS 181. If you find these problems challenging please check out the Section 0 document and/or reach out to one of the TFs. Even if you find this easy, we still recommend reading through the Section 0 notes for review. \end{center}

\begin{problem}
		    Given the matrix $\mathbf{X}$ and the vectors $\mathbf{y}$ and $\mathbf{z}$  below:
		    \begin{equation}
		        \mathbf{X} = \begin{pmatrix}
		        x_{11} & x_{12}\\
		        x_{21} & x_{22}
		        \end{pmatrix} \hspace{10pt} \mathbf{y} = \begin{pmatrix} y_{1} \\ y_{2} \end{pmatrix} \hspace{10pt} \mathbf{z} = \begin{pmatrix} z_{1} \\ z_{2} \end{pmatrix} \hspace{10pt} 
		    \end{equation}  
		    \begin{enumerate}[label=(\alph*)]
		        \item Expand $\mathbf{X}\mathbf{y} + \mathbf{z}$ 
		        
		        \item Expand $\mathbf{y^T}\mathbf{X}\mathbf{y}$

		    \end{enumerate}
		\textbf{Solution:} 
		\begin{enumerate}[label=(\alph*)]
		        \item 
		        \begin{equation}
		        \mathbf{X y + z} = \begin{pmatrix}
		                        x_{11}y_{1} + x_{12}y_{2} \\
		                        x_{21}y_{1} + x_{22}y_{2}
		                        \end{pmatrix} +  \begin{pmatrix}
		                        z_1 \\
		                        z_2
		                        \end{pmatrix} = \begin{pmatrix}
		                        x_{11}y_{1} + x_{12}y_{2} + z_1 \\
		                        x_{21}y_{1} + x_{22}y_{2} + z_2
		                        \end{pmatrix} \nonumber
		        \end{equation}
		        
		        \item 
		        \begin{align*}
		        \mathbf{y^TXy} &= \begin{pmatrix}
		                        y_1 & y_2
		                        \end{pmatrix} \begin{pmatrix}
		                        x_{11} & x_{12} \\
		                        x_{21} & x_{22}
		                        \end{pmatrix} \begin{pmatrix}
		                        y_1 \\ y_2
		                        \end{pmatrix}  \\
		        &= \begin{pmatrix}
		                        x_{11}y_{1} + x_{21}y_{2} &
		                        x_{12}y_{1} + x_{22}y_{2}
		                        \end{pmatrix}\begin{pmatrix}
		                        y_1 \\ y_2
		                        \end{pmatrix} \\
		       &= x_{11}y_1^2 + x_{21}y_1y_2 + x_{12}y_1y_2 + x_{22}y_2^2\\
		        \end{align*}

		    \end{enumerate}
		\end{problem}


\begin{problem}
Assume matrix $\mathbf{X}$ has dimensionality (or shape) $(n \times d)$, and vector $\mathbf{w}$ has shape $(d \times 1)$.

\begin{enumerate}[label=(\alph*)]
		        
		        \item What shape is $\mathbf{y} =  \mathbf{X} \mathbf{w}$?
		        
		        \item What shape is $(\mathbf{X}^T \mathbf{X})^{-1}$?
		        
		        \item Using $y$ from part (a), what shape is $(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T y$?
		        
		        \item Assume vector $\mathbf{w}' = \mathbf{w}^T$.  What shape is $\mathbf{y}' = \mathbf{X}\mathbf{w}'^T $?  

		    \end{enumerate}

\noindent \textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item $(n \times 1)$
    \item $\mathbf{X}^T\mathbf{X}$ has shape $(d \times d)$ and so $(\mathbf{X}^T\mathbf{X})^{-1}$ has shape $(d \times d)$
    \item $\mathbf{X}^T \mathbf{y}$ has shape $(d \times 1)$ and so $(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T \mathbf{y}$ has shape $(d \times 1)$
    \item Transposing a matrix twice returns the original matrix so we have $\mathbf{y}' = \mathbf X \mathbf{w}$, which has shape $(n  \times 1)$
\end{enumerate}
\end{problem}


\begin{problem}
        Write $\mathbf{u} = \mathbf{u}^\parallel + \mathbf{u^\perp}$ where $\mathbf{u}^\parallel = \frac{\langle \v u, \v v \rangle}
				{\langle \v v, \v v \rangle} \v v$ is the projection of $\v u$ onto $\v v$. Verify that $\langle \mathbf{u}^\parallel,
		\mathbf{u^\perp} \rangle = 0$ and that $\v u = \mathbf{u}^\parallel$ if and only if $\v u$ is a scaled multiple of $\v v$.
		\\ 
		\\
		\textbf{Solution:} We have $\mathbf{u}^\perp = \v u - \mathbf{u}^\parallel = \v u - \frac{\langle \v u, \v v \rangle}
				{\langle \v v, \v v \rangle} \v v.$ Then
				$$
				\langle \mathbf{u}^\parallel, \v u^\perp\rangle = \left\langle \frac{\langle \v u, \v v \rangle}
				{\langle \v v, \v v \rangle} \v v, \v u - \frac{\langle \v u, \v v \rangle}
				{\langle \v v, \v v \rangle} \v v \right\rangle = \frac{\langle \v u, \v v \rangle}
				{\langle \v v, \v v \rangle}\left\langle \v v, \v u - \frac{\langle \v u, \v v \rangle}
				{\langle \v v, \v v \rangle} \v v \right\rangle = $$ $$
				\frac{\langle \v u, \v v \rangle}
				{\langle \v v, \v v \rangle}\left(\langle \v v, \v u \rangle - \frac{\langle \v u, \v v \rangle}
				{\langle \v v, \v v \rangle}\langle \v v, \v v \rangle\right) = \frac{\langle \v u, \v v \rangle}
				{\langle \v v, \v v \rangle}(\langle \v v, \v u \rangle - \langle \v u, \v v \rangle) = \frac{\langle \v u, \v v \rangle}
				{\langle \v v, \v v \rangle}(\langle \v v, \v u \rangle - \langle \v v, \v u \rangle) = 0
				,$$
				where we note that $\langle \v v, \v u \rangle = \langle \v u, \v v\rangle$ since $\v u$ and $\v v$ are real vectors.\\
				\\
				If $\v u = \mathbf{u}^\parallel = \frac{\langle \v u, \v v \rangle}
				{\langle \v v, \v v \rangle}\v v$ then $\v u$ is a scaled multiple of $\v v$. For the other direction suppose $\v u = c \v v$ for some $c \in \R$. Then $\langle \v u, \v v \rangle = \langle c \v v, \v v \rangle = c \langle \v v, \v v \rangle \implies \mathbf{u}^\parallel = \frac{\langle \v u, \v v \rangle}{\langle \v v, \v v \rangle} \v v = \frac{c\langle \v v, \v v \rangle}{\langle \v v, \v v \rangle} \v v = c\v v = \v u. $
        \end{problem}


\begin{problem}
                For an invertible matrix $\mathbf{A}$ show that $|\mathbf{A}^{-1}| = \frac{1}{|\mathbf A|}$ where $|\mathbf A|$ is the determinant of $\mathbf{A}.$
                \\
                \\
                \textbf{Solution}: We have $\mathbf{AA^{-1}} = \mathbf I$ so $|\mathbf{AA^{-1}}| = |\mathbf A| \cdot |\mathbf{A^{-1}}| = |\mathbf{I}| = 1 \implies |\mathbf{A}^{-1}| = \frac{1}{|\mathbf A|}$, where we use the fact that the determinant factors over products and that $|\mathbf A| \neq 0$ since $\mathbf A$ is invertible.
\end{problem}
        
        
        

\begin{problem} 
		       Solve the following vector/matrix calculus problems. In all of the below, $\mathbf{x}$ and $\mathbf{w}$ are column vectors (i.e. $n \times 1$ vectors).  It may be helpful to refer to \href{https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf}{\emph{The Matrix Cookbook}} by Petersen and Pedersen, specifically sections 2.4, 2.6, and 2.7.
		    
		    \begin{enumerate} [label=(\alph*)]
		        \item Let $f(\mathbf{x}) = \mathbf{x}^T \mathbf{x}$. Find $\nabla_{\mathbf{x}} f(\mathbf{x}) = \frac{\delta}{\delta \mathbf{x}} f(\mathbf{x})$.
		        
		        \emph{Hint}: As a first step, you can expand $\mathbf{x}^T \mathbf{x} = (x_1^2 + x_2^2 + ... + x_n^2)$, where $\mathbf{x} = (x_1, ..., x_n)$. 
		        
		        \item Let $f(\mathbf{w}) = (1 - \mathbf{w}^T \mathbf{x})^2$. Find $\nabla_{\mathbf{w}} f(\mathbf{w}) = \frac{\delta}{\delta \mathbf{w}} f(\mathbf{w})$.
		        
		        % TODO I'm assuming this was the right gradient?
		        \item Let $\mathbf{A}$ be a symmetric $n$-by-$n$ matrix. If $f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^T \mathbf{A} \mathbf{x} + \mathbf{w}^T \mathbf{x}$, find $\nabla_{\mathbf{x}} f(\mathbf{x}) = \frac{\delta}{\delta \mathbf{x}} f(\mathbf{x})$.
		        \end{enumerate}
		    \textbf{Solution:}
		    
		    \begin{enumerate} [label=(\alph*)]
		        \item \begin{align*}
		            \nabla (\mathbf{x}^T\mathbf{x}) &= \nabla (x_{1}^2 + x_{2}^2 + \ldots + x_{n}^2) \\
		            &= \nabla (x_{1}^2 + x_{2}^2 + \ldots + x_{n}^2) \\
		            &= \begin{pmatrix} 2x_1 \\ 2x_2 \\ \vdots \\ 2x_n
		            \end{pmatrix} \\
		            &= 2\mathbf{x}
		        \end{align*}
		        
		        \item By the chain rule: \begin{align*}
		            \frac{\partial f}{\partial \mathbf{w}} &= 2(1-\mathbf{w}^T\mathbf{x}) \frac{\partial}{\partial \mathbf{w}} (1-\mathbf{w}^T\mathbf{x}) \\
		            &= -2(1-\mathbf{w}^T\mathbf{x})\mathbf{x}
		        \end{align*}
		        
		        \item The partial of $\mathbf{x^TAx}$ with respect to $x_i$ is: \begin{align*}
		            \frac{\partial}{\partial x_i} \mathbf{x^TAx} &= \frac{\partial}{\partial x_i} \sum_{j=1}^n \sum_{k=1}^n a_{jk} x_i x_j \\
		            &= \sum_{k\neq i}a_{ik} x_k +  \sum_{j\neq i}a_{ji}x_j + 2a_{ii}x_i \\ 
		            &= \sum_{k=1}^n a_{ik} x_k + \sum_{j=1}^n a_{ji} x_j \\
		            &= \sum_{k=1}^n a_{ik} x_k + \sum_{k=1}^n a_{ik} x_k \hspace{5pt} \text{since $A$ is symmetric}\\
		            &= 2\sum_{k=1}^n a_{ik} x_k
		        \end{align*}
		        
		        This is the $i$th row that results from multiplying $2Ax$. Thus $\frac{\partial}{\partial \mathbf{x}} \mathbf{x^TAx}$ is $2Ax$.
		        
		        Since $\frac{\partial}{\partial \mathbf{x}}\mathbf{w^Tx}$ is $\mathbf{w}$, the total answer is:
		        
		        \begin{equation}
		            2\mathbf{A}\mathbf{x} + \mathbf{w}
		        \end{equation}
		       
		    \end{enumerate}
 		    
		\end{problem}
        
\begin{problem} Solve the following: 
\begin{enumerate} [label=(\alph*)] 
\item Verify that $\var(aX + b) = a^2\var(X)$.

\emph{Hint}: As a first step, you can expand $Var(aX + b)$ using the definition of variance.  Simplify using properties of expectations.
\item Suppose that $X_1, ..., X_n$ are i.i.d with mean $\mu$ and variance $\sigma^2$. Let $\Bar{X}$ be the mean $\frac{1}{n}\sum_i^n X_i$. Find $E(\Bar{X})$ and $Var(\Bar{X})$.
\end{enumerate}
\textbf{Solution:} 
\begin{enumerate} [label=(\alph*)] 
\item We have
$$
\var(aX+b) = \E[(aX+b)^2] - (\E[aX+b])^2 = \E[a^2X^2 + 2abX + b^2] - (a\E[x] + b)^2 = $$ $$
a^2\E[X^2] + 2ab\E[X] + b^2 - a^2(\E[X])^2 -2ab\E[X] - b^2 = a^2(\E[X^2] - (\E[X])^2) = a^2\var(X)
.$$
\item 
\begin{align*}
    E(\Bar{X}) = E(\frac{1}{n}\sum_i^n X_i) = \frac{1}{n}\sum_i^n E(X_i) = \frac{1}{n}\sum_i^n \mu = \frac{1}{n} \cdot n \cdot \mu = \mu \\ 
    \var(\Bar{X}) = \var(\frac{1}{n}\sum_i^n X_i) = \frac{1}{n^2}\sum_i^n \var(X_i) = \frac{1}{n^2}\sum_i^n \sigma^2 = \frac{1}{n^2} \cdot n \cdot \sigma^2 = \frac{\sigma^2}{n}
\end{align*}
\end{enumerate}
\end{problem}
		
% \begin{problem}Show that if $X$ and $Y$ are independent then $p(x|y) = p(x)$. Interpret this. \end{problem}
		    
\begin{problem}
Prove or come up with counterexamples for the following statements:
    \begin{enumerate}[label=(\alph*)]
        \item  Random variables $A$ and $B$ are conditionally independent given $C$.  Does this imply that $A$ and $B$ are (unconditionally) independent?
        \item  Random variables $A$ and $B$ are independent.  Does this imply that $A$ and $B$ are conditionally independent given some random variable $C$?
    \end{enumerate}

\noindent \textbf{Solution:} 
(a) Now suppose we have a fair coin $C_1$ and an unfair coin $C_2$ that has Heads on both sides. We will select 1 coin and then flip the coin twice. Let $C$ be the event that we select $C_1$. Let $A$ be be the event that first flip lands Heads and let $B$ be the event that the second flip lands Heads. Given $C$ we have that $A$ and $B$ are two separate flips of a fair coin, and so the flips are independent given $C$. However, suppose we do not know which coin has been selected. Then given $A$ has occurred the probability of selecting coin $C_1$ is 1/3 and that of selecting coin $C_2$ is 2/3. But then $\mathbb P(B|A) = \mathbb P(B|C_1)\mathbb P(C_1|A) + \mathbb P(B|C_2)\mathbb P(C_2|A) = 1/3(1/2) + 2/3(1) = 5/6 \neq 3/4 = \mathbb P(B),$ so $A$ and $B$ are not independent.
\\
\\
(b) No! First, consider two fair, independent coin flips $A, B$. Let $C$ be the event that $A=B$. On their own, $A$ and $B$ are independent but given $C$ we can determine $B$ from $A$ or $A$ from $B$ (they are either perfectly correlated or perfectly anti-correlated), so $A$ and $B$ are not conditionally independent given $C$.
\end{problem}
		   
\begin{problem}
    Suppose you undergo a test for a disease  whose frequency in the population is 1\% (i.e. the probability of any given person having the disease is 1\%). The test for the disease has a 5\% false positive rate (i.e. given that you don't have the disease, there's a 5\% chance you test positive) and a 10\% false negative rate (i.e. given that you do have the disease, there's a 10\% chance that you test negative). Suppose you take the test and it comes back positive. What is the probability that you have the disease? 
    \\
    \\
    \noindent \textbf{Solution:} 
    Let $D$ be the event that you have the disease and $T$ be the event that you test positive for the disease.
    \\
    Applying Bayes' Rule and the law of total probability:
    \begin{align*}
        P(D|T) &= \frac{P(T|D)P(D)}{P(T)} \\
        &= \frac{P(T|D)P(D)}{P(T|D)P(D) + P(T|D^c)P(D^c)} \\
        &= \frac{0.90 \cdot 0.01}{0.90 \cdot 0.01 + 0.05 \cdot 0.99} \\
        &\approx 0.15
    \end{align*}
\end{problem}





\begin{problem}
    Show that for random variables $X, Y$ that $\var(X+Y) = \var(X) + \var(Y) + 2\cov(X, Y)$.
    \\
    \\
    \textbf{Solution:} We have
    $$
    \var(X+Y) = \E[(X+Y)^2] - (\E[X+Y])^2 = \E[X^2 + 2XY + Y^2] - (\E[X]+\E[Y])^2 = $$ $$ \E[X^2]-(\E[X])^2 + \E[Y^2]-(\E[Y])^2 + 2(\E[XY]-\E[X]\E[Y]) = \var(X) + \var(Y) + 2\cov(X, Y)
    .$$
    \end{problem}
    
\begin{problem}
Using the probability density function of $X \sim \mathcal{N}(0, 1)$ show that $X$ has mean $0$ and variance $1$. \\
\\
\emph{Hint}: The PDF is $p(x) = \frac{1}{\sqrt{2\pi}} 
				\exp\left( -\frac{1}{2} x^2 \right).$ For the mean, you can reason about the properties of the PDF itself to get the answer without integration techniques. For the variance, use integration by parts with $u=x$ and $dv = xe^{-x^2/2} dx$ and the fact that the PDF itself integrates to $1$.
\\
\\
\textbf{Solution:} For the mean, first write down the expression for the mean: 
\begin{align*}
    E(X) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} xe^{-\frac{x^2}{2}}
\end{align*}
Notice that the PDF is an odd function, (i.e. $f(-x) = -f(x)$), and so the area from $-\infty$ to $0$ exactly cancels out the area from $0$ to $\infty$. Thus, $E(X) = 0$.
\newline
For the variance, we use LOTUS:
\begin{align*}
    Var(X) &= E(X^2) - (EX)^2 = E(X^2) \\
    &= \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} x^2e^{-\frac{x^2}{2}} \\
        &= \frac{2}{\sqrt{2\pi}} \int_{0}^{\infty} x^2e^{-\frac{x^2}{2}} \hspace{5pt} \text{since $x^2e^{-x^2/2}$ is an even function}\\
\end{align*}
Now use integration by parts with $u = x$ and $dv = xe^{-x^2/2}dx$, so $du = dx$ and $v = -e^{-x^2/2}$:
\begin{align*}
    Var(X) &= \frac{2}{\sqrt{2\pi}} (-x e^{-\frac{x^2}{2}}\Big|_0^{\infty} + \int_{0}^{\infty} e^{-\frac{x^2}{2}})\\
    &= \frac{2}{\sqrt{2\pi}}(0 + \frac{\sqrt{2\pi}}{2}) \\
    &= 1
\end{align*}
When going from the first to the second line, we use the fact that the second term is the normal PDF over half the support and without the $\frac{1}{\sqrt{2\pi}}$ term. 
\end{problem}


\begin{problem}
% Conditional and joint probabilities, continuous case (integrating to marginalize)

A random point $(X, Y, Z)$ is chosen uniformly in the ball 
$$B = \{(x, y, z): x^2 + y^2 + z^2 \leq 1\}$$

\begin{enumerate} [label=(\alph*)] 
\item Find the joint PDF of $(X, Y, Z)$.
\item Find the joint PDF of $(X, Y)$.
\item Write an expression for the maginal PDF of $X$, as an integral.

\end{enumerate}

\noindent\textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item Let $B = \{(x, y, z) \ | \ x^2 + y^2 + z^2 \leq 1\}$ be the closed unit ball. The volume of $B$ is $\text{vol}(B) = \int_B 1 dx \ dy \ dz = \frac{4}{3}\pi$. Since the distribution of $(X, Y, Z)$ is uniform over $B$ the PDF is then $$f(x, y, z) = \frac{3}{4\pi}\cdot\chi((x, y, z) \in B)$$
    where $\chi((x, y, z) \in B) = \begin{cases}
    1 & (x, y, z) \in B \\
    0 & (x, y, z) \not\in B
    \end{cases}$
    \item Let $C = \{(x, y) \ | x^2 + y^2 \leq 1\}$ be the unit circle. We have
    $$
    f(x, y) = \int_{\mathbb R} f(x, y, z) \ dz = \frac{3}{4\pi} \int_{\mathbb R}\chi((x, y, z) \in B) \ dz = \frac{3}{4\pi} \int_{-\sqrt{1-x^2-y^2}}^{\sqrt{1-x^2-y^2}} \chi((x, y) \in C) \ dz = $$ $$\frac{3}{2\pi} \sqrt{1-x^2-y^2} \cdot \chi((x, y) \in C)
    .$$
    \item We have
    $$
    f(x) = \int_{\mathbb R} f(x, y) \ dy = \frac{3}{2\pi}\int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}} \sqrt{1-x^2-y^2} \cdot \chi(x \in [-1, 1]) \ dy
    .$$
\end{enumerate}
\end{problem}

\begin{problem}
% Binary conditional and joint probabilities

Suppose we randomly sample a Harvard College student from the undergraduate population.  Let $X$ be the indicator of the sampled individual concentrating in computer science, and let $Y$ be the indicator of their working in the tech industry after graduation.\\

Suppose that the below table represented the joint PMF of $X$ and $Y$:

\begin{center}
\begin{tabular}{ c | c c }
  & $Y = 1$ & $Y = 0$ \\ \hline\\
 $X = 1$ & $\frac{10}{100}$ & $\frac{5}{100}$ \\  \\
 $X = 0$ & $\frac{15}{100}$ & $\frac{70}{100}$ \\   
\end{tabular}
\end{center}

\begin{enumerate}[label=(\alph*)] 
\item Calculate marginal probability $P(Y = 1)$.  In the context of this problem, what does this probability represent?
\item Calculate conditional probability $P(Y = 1 | X = 1)$.  In the context of this problem, what does this probability represent?
\item Are $X$ and $Y$ independent?  Why or why not?

\end{enumerate}

\noindent \textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item We have $P(Y=1) = P(Y=1, X=1) + P(Y=1, X=0) = \frac{10}{100} + \frac{15}{100} = \frac{1}{4}$. This represents the probability that a Harvard student works in the tech industry after graduation.
    \item Similarly, we compute $P(X=1) = P(X=1, Y=1) + P(X=1, Y=0) = \frac{10}{100} + \frac{5}{100} = \frac{3}{20}$. Then we compute $P(Y=1 | X=1) = \frac{P(Y=1, X=1)}{P(X=1)} = \frac{10/100}{3/20} = \frac{2}{3}.$ This represents the probability of a CS concentrator at Harvard working in the tech industry after graduation.
    \item $X$ and $Y$ are not independent since $P(Y=1|X=1) \neq P(Y=1).$
\end{enumerate}

\end{problem}

\begin{problem}
% Single-variable convex optimization overview

In her most recent work-from-home shopping spree, Nari decided to buy several house plants.  She would like for them to grow as tall as possible, but needs your calculus help to understand how to best take care of them.

\begin{enumerate} [label=(\alph*)] 
\item After perusing the internet, Nari learns that the height $y$ in mm of her Weeping Fig plant can be directly modeled as a function of the oz of water $x$ she gives it each week:

%TODO find something easily differentiable with only positive support. (there's nothing convex though, hmm.  it would be easiest to just make the question itself something with infinite negative support .....


$$ y = - 3x^2 + 72x + 70$$

Is this function concave, convex, or neither?  Explain why or why not.

\item Solve analytically for the critical points of this expression.  For each critical point, use the second-derivative test to identify if each point is a local max, global max, local min, or global min. 

\item  How many oz per week should Nari water her plant to maximize its height? With this much water how tall will her plant grow?

\item Nari also has a Money Tree plant.  The height $y$ in mm of her Money Tree can be directly modeled as a function of the oz of water $x$ she gives it per week:

$$ y = - x^4 + 16 x^3 - 93 x^2 + 230 x - 190$$

Is this function concave, convex, or neither?  Explain why or why not.

\end{enumerate}

\noindent \textbf{Solution:}
\begin{enumerate}[label=(\alph*)]
    \item It is concave since the 2nd derivative is $y'' = -6 < 0$.
    \item The first derivative is $y' = -6x+72 = -6(x-12)$. We have $y' = 0$ if and only if $x = 12$, so $x=12$ is the only critical point. Since $y' > 0$ for $x < 12$ and $y' < 0$ for $x > 12$ we know that $x=12$ is a local maximum. Since $y$ is concave ($y'' < 0$) the point $x=12$ is the global maximum.
    \item She should give her plant $12$ oz of water a week for it to achieve the maximum height of $502$ mm.
    \item Neither, the 2nd derivative is $y'' = -12x^2 + 96x -186$, which is negative and positive depending on $x$.
\end{enumerate}


\end{problem}


\noindent Credits:  Problems 11 and 12 were inspired by Exercise 7.19 and  Example 7.1.5 in Blitzstein \& Hwang's ``Introduction to Probability''.

\end{document}